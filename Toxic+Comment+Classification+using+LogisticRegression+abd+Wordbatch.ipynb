{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Reading input files] done in 2 s\n",
      "[Performing basic NLP] done in 96 s\n",
      "[Creating numerical features] done in 2 s\n",
      "[Tfidf on word] done in 80 s\n",
      "[Tfidf on char n_gram] done in 411 s\n",
      "[Staking matrices] done in 15 s\n",
      "Shapes just to be sure :  (159571, 70127) (153164, 70127)\n",
      "CV score for class toxic           is full 0.979486 | mean 0.979523+0.000428\n",
      "CV score for class severe_toxic    is full 0.985516 | mean 0.985771+0.004069\n",
      "CV score for class obscene         is full 0.991965 | mean 0.991982+0.000840\n",
      "CV score for class threat          is full 0.982086 | mean 0.982300+0.008016\n",
      "CV score for class insult          is full 0.983721 | mean 0.983730+0.000914\n",
      "CV score for class identity_hate   is full 0.971193 | mean 0.971253+0.002522\n",
      "Total CV score is 0.982328+0.001037\n",
      "[Scoring LogisticRegression] done in 5513 s\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from contextlib import contextmanager\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "import time\n",
    "\n",
    "import regex as re\n",
    "\n",
    "import string\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "\n",
    "from wordbatch.models import FTRL, FM_FTRL\n",
    "\n",
    "import gc\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "cont_patterns = [\n",
    "\n",
    "        (b'US', b'United States'),\n",
    "\n",
    "        (b'IT', b'Information Technology'),\n",
    "\n",
    "        (b'(W|w)on\\'t', b'will not'),\n",
    "\n",
    "        (b'(C|c)an\\'t', b'can not'),\n",
    "\n",
    "        (b'(I|i)\\'m', b'i am'),\n",
    "\n",
    "        (b'(A|a)in\\'t', b'is not'),\n",
    "\n",
    "        (b'(\\w+)\\'ll', b'\\g<1> will'),\n",
    "\n",
    "        (b'(\\w+)n\\'t', b'\\g<1> not'),\n",
    "\n",
    "        (b'(\\w+)\\'ve', b'\\g<1> have'),\n",
    "\n",
    "        (b'(\\w+)\\'s', b'\\g<1> is'),\n",
    "\n",
    "        (b'(\\w+)\\'re', b'\\g<1> are'),\n",
    "\n",
    "        (b'(\\w+)\\'d', b'\\g<1> would'),\n",
    "\n",
    "    ]\n",
    "\n",
    "patterns = [(re.compile(regex), repl) for (regex, repl) in cont_patterns]\n",
    "\n",
    "\n",
    "\n",
    "def prepare_for_char_n_gram(text):\n",
    "\n",
    "    \"\"\" Simple text clean up process\"\"\"\n",
    "\n",
    "    # 1. Go to lower case (only good for english)\n",
    "\n",
    "    # Go to bytes_strings as I had issues removing all \\n in r\"\"\n",
    "\n",
    "    clean = bytes(text.lower(), encoding=\"utf-8\")\n",
    "\n",
    "    # 2. Drop \\n and  \\t\n",
    "\n",
    "    clean = clean.replace(b\"\\n\", b\" \")\n",
    "\n",
    "    clean = clean.replace(b\"\\t\", b\" \")\n",
    "\n",
    "    clean = clean.replace(b\"\\b\", b\" \")\n",
    "\n",
    "    clean = clean.replace(b\"\\r\", b\" \")\n",
    "\n",
    "    # 3. Replace english contractions\n",
    "\n",
    "    for (pattern, repl) in patterns:\n",
    "\n",
    "        clean = re.sub(pattern, repl, clean)\n",
    "\n",
    "    # 4. Drop puntuation\n",
    "\n",
    "    # I could have used regex package with regex.sub(b\"\\p{P}\", \" \")\n",
    "\n",
    "    exclude = re.compile(b'[%s]' % re.escape(bytes(string.punctuation, encoding='utf-8')))\n",
    "\n",
    "    clean = b\" \".join([exclude.sub(b'', token) for token in clean.split()])\n",
    "\n",
    "    # 5. Drop numbers - as a scientist I don't think numbers are toxic ;-)\n",
    "\n",
    "    clean = re.sub(b\"\\d+\", b\" \", clean)\n",
    "\n",
    "    # 6. Remove extra spaces - At the end of previous operations we multiplied space accurences\n",
    "\n",
    "    clean = re.sub(b'\\s+', b' ', clean)\n",
    "\n",
    "    # Remove ending space if any\n",
    "\n",
    "    clean = re.sub(b'\\s+$', b'', clean)\n",
    "\n",
    "    # 7. Now replace words by words surrounded by # signs\n",
    "\n",
    "    # e.g. my name is bond would become #my# #name# #is# #bond#\n",
    "\n",
    "    # clean = re.sub(b\"([a-z]+)\", b\"#\\g<1>#\", clean)\n",
    "\n",
    "    clean = re.sub(b\" \", b\"# #\", clean)  # Replace space\n",
    "\n",
    "    clean = b\"#\" + clean + b\"#\"  # add leading and trailing #\n",
    "\n",
    "\n",
    "\n",
    "    return str(clean, 'utf-8')\n",
    "\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "\n",
    "def timer(name):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Taken from Konstantin Lopuhin https://www.kaggle.com/lopuhin\n",
    "\n",
    "    in script named : Mercari Golf: 0.3875 CV in 75 LOC, 1900 s\n",
    "\n",
    "    https://www.kaggle.com/lopuhin/mercari-golf-0-3875-cv-in-75-loc-1900-s\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    yield\n",
    "\n",
    "    print(f'[{name}] done in {time.time() - t0:.0f} s')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def count_regexp_occ(regexp=\"\", text=None):\n",
    "\n",
    "    \"\"\" Simple way to get the number of occurence of a regex\"\"\"\n",
    "\n",
    "    return len(re.findall(regexp, text))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_indicators_and_clean_comments(df):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Check all sorts of content as it may help find toxic comment\n",
    "\n",
    "    Though I'm not sure all of them improve scores\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Count number of \\n\n",
    "\n",
    "    df[\"ant_slash_n\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\n\", x))\n",
    "\n",
    "    # Get length in words and characters\n",
    "\n",
    "    df[\"raw_word_len\"] = df[\"comment_text\"].apply(lambda x: len(x.split()))\n",
    "\n",
    "    df[\"raw_char_len\"] = df[\"comment_text\"].apply(lambda x: len(x))\n",
    "\n",
    "    # Check number of upper case, if you're angry you may write in upper case\n",
    "\n",
    "    df[\"nb_upper\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"[A-Z]\", x))\n",
    "\n",
    "    # Number of F words - f..k contains folk, fork,\n",
    "\n",
    "    df[\"nb_fk\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"[Ff]\\S{2}[Kk]\", x))\n",
    "\n",
    "    # Number of S word\n",
    "\n",
    "    df[\"nb_sk\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"[Ss]\\S{2}[Kk]\", x))\n",
    "\n",
    "    # Number of D words\n",
    "\n",
    "    df[\"nb_dk\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"[dD]ick\", x))\n",
    "\n",
    "    # Number of occurence of You, insulting someone usually needs someone called : you\n",
    "\n",
    "    df[\"nb_you\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\W[Yy]ou\\W\", x))\n",
    "\n",
    "    # Just to check you really refered to my mother ;-)\n",
    "\n",
    "    df[\"nb_mother\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\Wmother\\W\", x))\n",
    "\n",
    "    # Just checking for toxic 19th century vocabulary\n",
    "\n",
    "    df[\"nb_ng\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\Wnigger\\W\", x))\n",
    "\n",
    "    # Some Sentences start with a <:> so it may help\n",
    "\n",
    "    df[\"start_with_columns\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"^\\:+\", x))\n",
    "\n",
    "    # Check for time stamp\n",
    "\n",
    "    df[\"has_timestamp\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\d{2}|:\\d{2}\", x))\n",
    "\n",
    "    # Check for dates 18:44, 8 December 2010\n",
    "\n",
    "    df[\"has_date_long\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\D\\d{2}:\\d{2}, \\d{1,2} \\w+ \\d{4}\", x))\n",
    "\n",
    "    # Check for date short 8 December 2010\n",
    "\n",
    "    df[\"has_date_short\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\D\\d{1,2} \\w+ \\d{4}\", x))\n",
    "\n",
    "    # Check for http links\n",
    "\n",
    "    df[\"has_http\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"http[s]{0,1}://\\S+\", x))\n",
    "\n",
    "    # check for mail\n",
    "\n",
    "    df[\"has_mail\"] = df[\"comment_text\"].apply(\n",
    "\n",
    "        lambda x: count_regexp_occ(r'[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+', x)\n",
    "\n",
    "    )\n",
    "\n",
    "    # Looking for words surrounded by == word == or \"\"\"\" word \"\"\"\"\n",
    "\n",
    "    df[\"has_emphasize_equal\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\={2}.+\\={2}\", x))\n",
    "\n",
    "    df[\"has_emphasize_quotes\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\\"{4}\\S+\\\"{4}\", x))\n",
    "\n",
    "\n",
    "\n",
    "    # Now clean comments\n",
    "\n",
    "    df[\"clean_comment\"] = df[\"comment_text\"].apply(lambda x: prepare_for_char_n_gram(x))\n",
    "\n",
    "\n",
    "\n",
    "    # Get the new length in words and characters\n",
    "\n",
    "    df[\"clean_word_len\"] = df[\"clean_comment\"].apply(lambda x: len(x.split()))\n",
    "\n",
    "    df[\"clean_char_len\"] = df[\"clean_comment\"].apply(lambda x: len(x))\n",
    "\n",
    "    # Number of different characters used in a comment\n",
    "\n",
    "    # Using the f word only will reduce the number of letters required in the comment\n",
    "\n",
    "    df[\"clean_chars\"] = df[\"clean_comment\"].apply(lambda x: len(set(x)))\n",
    "\n",
    "    df[\"clean_chars_ratio\"] = df[\"clean_comment\"].apply(lambda x: len(set(x))) / df[\"clean_comment\"].apply(\n",
    "\n",
    "        lambda x: 1 + min(99, len(x)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def char_analyzer(text):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    This is used to split strings in small lots\n",
    "\n",
    "    I saw this in an article (I can't find the link anymore)\n",
    "\n",
    "    so <talk> and <talking> would have <Tal> <alk> in common\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    tokens = text.split()\n",
    "\n",
    "    return [token[i: i + 3] for token in tokens for i in range(len(token) - 2)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def clean_csr(csr_trn, csr_sub, min_samples):\n",
    "\n",
    "    idx = np.arange(csr_trn.shape[1])\n",
    "\n",
    "    trn_min = np.array((csr_trn.sum(axis=0) > min_samples))[0]\n",
    "\n",
    "    sub_min = np.array((csr_sub.sum(axis=0) > min_samples))[0]\n",
    "\n",
    "    csr_trn2 = csr_trn[:, trn_min & sub_min]\n",
    "\n",
    "    del csr_trn\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    csr_sub2 = csr_sub[:, trn_min & sub_min]\n",
    "\n",
    "    del csr_sub\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "\n",
    "    return csr_trn2, csr_sub2\n",
    "\n",
    "\n",
    "\n",
    "def get_numerical_features(trn, sub):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    As @bangda suggested FM_FTRL either needs to scaled output or dummies\n",
    "\n",
    "    So here we go for dummies\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    ohe = OneHotEncoder()\n",
    "\n",
    "    full_csr = ohe.fit_transform(np.vstack((trn.values, sub.values)))\n",
    "\n",
    "    csr_trn = full_csr[:trn.shape[0]]\n",
    "\n",
    "    csr_sub = full_csr[trn.shape[0]:]\n",
    "\n",
    "    del full_csr\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    \n",
    "\n",
    "    # Now remove features that don't have enough samples either in train or test\n",
    "\n",
    "    \n",
    "\n",
    "    return clean_csr(csr_trn, csr_sub, 3)\n",
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "\n",
    "\n",
    "    class_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "\n",
    "\n",
    "    with timer(\"Reading input files\"):\n",
    "\n",
    "        train = pd.read_csv('E:/pg/docs/BPB/data/classification/toxic/train.csv').fillna(' ')\n",
    "\n",
    "        test = pd.read_csv('E:/pg/docs/BPB/data/classification/toxic/test.csv').fillna(' ')\n",
    "\n",
    "\n",
    "\n",
    "    with timer(\"Performing basic NLP\"):\n",
    "\n",
    "        for df in [train, test]:\n",
    "\n",
    "           get_indicators_and_clean_comments(df)\n",
    "\n",
    "        \n",
    "\n",
    "    train_text = train['clean_comment'].fillna(\"\")\n",
    "\n",
    "    test_text = test['clean_comment'].fillna(\"\")\n",
    "\n",
    "    all_text = pd.concat([train_text, test_text])\n",
    "\n",
    "\n",
    "\n",
    "    with timer(\"Creating numerical features\"):\n",
    "\n",
    "        num_features = [f_ for f_ in train.columns\n",
    "\n",
    "                        if f_ not in [\"comment_text\", \"clean_comment\", \"id\", \"remaining_chars\", 'has_ip_address'] + class_names]\n",
    "\n",
    "\n",
    "\n",
    "        # skl = MinMaxScaler()\n",
    "\n",
    "        # skl.fit(pd.concat([train[num_features], test[num_features]], axis=0))\n",
    "\n",
    "        # train[num_features] = skl.transform(train[num_features])\n",
    "\n",
    "        # test[num_features] = skl.transform(test[num_features])\n",
    "\n",
    "        # FM_FTRL requires categorical data \n",
    "\n",
    "        for f in num_features:\n",
    "\n",
    "            all_cut = pd.cut(pd.concat([train[f], test[f]], axis=0), bins=20, labels=False, retbins=False)\n",
    "\n",
    "            train[f] = all_cut.values[:train.shape[0]]\n",
    "\n",
    "            test[f] = all_cut.values[train.shape[0]:]\n",
    "\n",
    "        \n",
    "\n",
    "        train_num_features, test_num_features = get_numerical_features(train[num_features], test[num_features])\n",
    "\n",
    "        # train_num_features = csr_matrix(train[num_features])\n",
    "\n",
    "        # test_num_features = csr_matrix(test[num_features])\n",
    "\n",
    "\n",
    "\n",
    "    with timer(\"Tfidf on word\"):\n",
    "\n",
    "        word_vectorizer = TfidfVectorizer(\n",
    "\n",
    "            sublinear_tf=True,\n",
    "\n",
    "            strip_accents='unicode',\n",
    "\n",
    "            tokenizer=lambda x: re.findall(r'[^\\p{P}\\W]+', x),\n",
    "\n",
    "            analyzer='word',\n",
    "\n",
    "            token_pattern=None,\n",
    "\n",
    "            stop_words='english',\n",
    "\n",
    "            ngram_range=(1, 2),  # was (1 , 2)\n",
    "\n",
    "            max_features=20000)\n",
    "\n",
    "        word_vectorizer.fit(all_text)\n",
    "\n",
    "        train_word_features = word_vectorizer.transform(train_text)\n",
    "\n",
    "        test_word_features = word_vectorizer.transform(test_text)\n",
    "\n",
    "\n",
    "\n",
    "    with timer(\"Tfidf on char n_gram\"):\n",
    "\n",
    "        char_vectorizer = TfidfVectorizer(\n",
    "\n",
    "            sublinear_tf=True,\n",
    "\n",
    "            strip_accents='unicode',\n",
    "\n",
    "            tokenizer=char_analyzer,\n",
    "\n",
    "            analyzer='word',\n",
    "\n",
    "            ngram_range=(1, 3),\n",
    "\n",
    "            max_features=50000)\n",
    "\n",
    "        char_vectorizer.fit(all_text)\n",
    "\n",
    "        train_char_features = char_vectorizer.transform(train_text)\n",
    "\n",
    "        test_char_features = char_vectorizer.transform(test_text)\n",
    "\n",
    "\n",
    "\n",
    "    with timer(\"Staking matrices\"):\n",
    "\n",
    "        train_features = hstack(\n",
    "\n",
    "            [\n",
    "\n",
    "                train_char_features,\n",
    "\n",
    "                train_word_features, \n",
    "\n",
    "                train_num_features\n",
    "\n",
    "            ]\n",
    "\n",
    "        ).tocsr()\n",
    "\n",
    "        del train_word_features, train_num_features, train_char_features\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "        \n",
    "\n",
    "        test_features = hstack(\n",
    "\n",
    "            [\n",
    "\n",
    "                test_char_features, \n",
    "\n",
    "                test_word_features, \n",
    "\n",
    "                test_num_features\n",
    "\n",
    "            ]\n",
    "\n",
    "        ).tocsr()\n",
    "\n",
    "        del test_word_features, test_num_features, test_char_features\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "    \n",
    "\n",
    "    print(\"Shapes just to be sure : \", train_features.shape, test_features.shape)\n",
    "\n",
    "    \n",
    "\n",
    "    f_range = (1e-6, 1 - 1e-6)\n",
    "\n",
    "    with timer(\"Scoring LogisticRegression\"):\n",
    "\n",
    "        folds = KFold(n_splits=4, shuffle=True, random_state=1)\n",
    "\n",
    "        losses = []\n",
    "\n",
    "        losses_per_folds = np.zeros(folds.n_splits)\n",
    "\n",
    "        submission = pd.DataFrame.from_dict({'id': test['id']})\n",
    "\n",
    "        for i_c, class_name in enumerate(class_names):\n",
    "\n",
    "            class_pred = np.zeros(len(train))\n",
    "\n",
    "            train_target = train[class_name]\n",
    "\n",
    "            submission[class_name] = 0.0\n",
    "\n",
    "            cv_scores = []\n",
    "\n",
    "            for n_fold, (trn_idx, val_idx) in enumerate(folds.split(train_features)):\n",
    "\n",
    "                # I know it's strange but I find MSE to be better here...\n",
    "\n",
    "                clf = FM_FTRL(\n",
    "\n",
    "                    alpha=0.01, beta=0.01, L1=0.00001, L2=0.1,\n",
    "\n",
    "                    D=train_features.shape[1], alpha_fm=0.01, \n",
    "\n",
    "                    L2_fm=0.0, init_fm=0.01,\n",
    "\n",
    "                    D_fm=200, e_noise=0.0001, iters=3, \n",
    "\n",
    "                    inv_link=\"identity\", threads=4\n",
    "\n",
    "                )\n",
    "\n",
    "                clf.fit(train_features[trn_idx], train_target.iloc[trn_idx])\n",
    "\n",
    "                # Compute prediction and use sigmoid\n",
    "\n",
    "                class_pred[val_idx] = sigmoid(clf.predict(train_features[val_idx]))\n",
    "\n",
    "                score = roc_auc_score(train_target.iloc[val_idx], class_pred[val_idx])\n",
    "\n",
    "                cv_scores.append(score)\n",
    "\n",
    "                losses_per_folds[n_fold] += score / len(class_names)\n",
    "\n",
    "                # Compute test predictions\n",
    "\n",
    "                submission[class_name + \"_temp\"] = clf.predict(test_features)\n",
    "\n",
    "                # Compute mean (without NaN)\n",
    "\n",
    "                class_mean = submission[class_name + \"_temp\"].mean()\n",
    "\n",
    "                # Replace NaNs if any\n",
    "\n",
    "                submission[class_name + \"_temp\"].fillna(class_mean, inplace=True)\n",
    "\n",
    "                # Transform using sigmoid\n",
    "\n",
    "                submission[class_name] += sigmoid(submission[class_name + \"_temp\"]) / folds.n_splits\n",
    "\n",
    "                del submission[class_name + \"_temp\"]\n",
    "\n",
    "            cv_score = roc_auc_score(train_target, class_pred)\n",
    "\n",
    "            losses.append(cv_score)\n",
    "\n",
    "            train[class_name + \"_oof\"] = class_pred\n",
    "\n",
    "            print('CV score for class %-15s is full %.6f | mean %.6f+%.6f'\n",
    "\n",
    "                  % (class_name, cv_score, np.mean(cv_scores), np.std(cv_scores)))\n",
    "\n",
    "        print('Total CV score is %.6f+%.6f' %(np.mean(losses), np.std(losses_per_folds)))\n",
    "\n",
    "        \n",
    "\n",
    "        train[[\"id\"] + class_names + [f + \"_oof\" for f in class_names]].to_csv(\"lvl0_wordbatch_clean_oof.csv\",\n",
    "\n",
    "                                                                               index=False,\n",
    "\n",
    "                                                                               float_format=\"%.8f\")\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "submission.to_csv(\"E:/pg/docs/BPB/data/classification/toxic/lvl0_wordbatch_clean_sub.csv\", index=False, float_format=\"%.8f\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
